# **LLaMA 3.2 3B Fine-Tuning with LoRA and GGUF Conversion**

## **ğŸ“Œ Overview**
This project fine-tunes the **LLaMA 3.2 3B model** using **LoRA (Low-Rank Adaptation)** to create an efficient, customized AI model. The fine-tuned model is then **merged and converted to GGUF format** for deployment in **Ollama**.

## **ğŸ›  Features**
âœ… **Efficient LoRA Fine-Tuning** â†’ Saves GPU memory and accelerates training.  
âœ… **4-bit Quantization** â†’ Enables fine-tuning on consumer GPUs (e.g., Tesla T4).  
âœ… **Log Summarization** â†’ AI-driven search and summarization of SMTP logs.  
âœ… **Merge LoRA with Base Model** â†’ No need to load adapters separately.  
âœ… **GGUF Conversion for Ollama** â†’ Fast and optimized model deployment.  

## **ğŸš€ Installation**
### **1ï¸âƒ£ Install Dependencies**
```bash
pip install -q pip3-autoremove
pip-autoremove torch torchvision torchaudio -y
pip install -q torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121
pip install -q unsloth
pip install -q --upgrade --no-cache-dir transformers datasets
```

### **2ï¸âƒ£ Download or Clone Repository**
```bash
git clone https://github.com/your-repo/llama-finetuning.git
cd llama-finetuning
```

## **ğŸ“‚ Project Structure**
```
â”œâ”€â”€ training.py                # Fine-tuning script for LLaMA 3.2 3B with LoRA
â”œâ”€â”€ merge_lora.py              # Merges LoRA fine-tuned adapters into the base model
â”œâ”€â”€ convert_to_gguf.py         # Converts the merged model into GGUF format
â”œâ”€â”€ requirements.txt           # List of dependencies
â””â”€â”€ README.md                  # Documentation
```

## **ğŸ”¹ Training Process**
### **1ï¸âƒ£ Load and Preprocess the Dataset**
- Loads logs and structures them for fine-tuning.
- Supports JSON datasets with conversations.

### **2ï¸âƒ£ Load Pretrained LLaMA Model**
- Uses **Unsloth** to efficiently load LLaMA 3.2 3B.
- Implements **4-bit quantization** for memory efficiency.

### **3ï¸âƒ£ Apply LoRA for Fine-Tuning**
- Fine-tunes **only selected model layers** to reduce GPU load.
```python
model = FastLanguageModel.get_peft_model(
    model,
    r=16,  # LoRA Rank
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_alpha=16,
    lora_dropout=0,
    bias="none",
)
```

### **4ï¸âƒ£ Train the Model**
- Uses **gradient accumulation** for better training efficiency.
```python
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    dataset_text_field="text",
)
trainer.train()
```

### **5ï¸âƒ£ Merge LoRA into the Base Model**
```python
model.load_adapter("/content/lora_model")
model.save_pretrained_merged("/content/merged_model", tokenizer, save_method="merged_16bit")
```

### **6ï¸âƒ£ Convert Model to GGUF Format for Ollama**
```python
model.save_pretrained_gguf("/content/ollama_model", tokenizer)
```

## **ğŸš€ Deployment with Ollama**
### **1ï¸âƒ£ Transfer the GGUF Model to Your Server**
```bash
scp ollama_model.zip user@your-server-ip:/root/.ollama/models/
```

### **2ï¸âƒ£ Register the Model in Ollama**
```bash
nano /root/.ollama/models/ollama_model/Modelfile
```
```
FROM /root/.ollama/models/ollama_model/unsloth.Q8_0.gguf
```
```bash
ollama create my_llama -f /root/.ollama/models/ollama_model/Modelfile
```

### **3ï¸âƒ£ Run the Model in Ollama**
```bash
ollama run my_llama
```

## **ğŸ“Œ Future Improvements**
- **Integrate LangChain for better log retrieval.**
- **Use FAISS for fast similarity search in logs.**
- **Deploy as an API using FastAPI for real-time summarization.**


---

ğŸ¯ **Need help? Reach out via GitHub Issues!** ğŸš€

